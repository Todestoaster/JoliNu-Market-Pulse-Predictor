{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b733257c-b3ec-41fb-8f3f-ee6c22be9788",
   "metadata": {},
   "source": [
    "# CNBC Crawler\n",
    "\n",
    "Basierend auf den Inhalten, die beim Aufruf der Seite https://www.cnbc.com/search/?query=boeing&qsearchterm=boeing geladen werden, wird die Schnittstelle direkt mittels Requests aufgerufen um die Daten zu laden. \n",
    "\n",
    "Der untenstehende Code ruft direkt api.queryly.com mit den passenden Kennwerten auf. Um das Netzwerk nicht zu belasten und eine Sperre inkaufnehemen zu müssen, wird der Code in Intervallen aufgerufen. Dabei wird nach jedem mal laden von 10 Inhalten, eine Pause zwischen 2-10 Sekunden gehalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcefad-d23f-45a3-be5a-0ce22af60eda",
   "metadata": {},
   "source": [
    "![alt text](screenshots/1.png \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fa0b1dc-2d59-426e-9746-75c8e2551fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 von 1193 Durchgängen ausgeführt. 10 Artikel geladen.\n",
      "Warte 6 Sekunden...\n",
      "2 von 1193 Durchgängen ausgeführt. 20 Artikel geladen.\n",
      "Warte 4 Sekunden...\n",
      "3 von 1193 Durchgängen ausgeführt. 30 Artikel geladen.\n",
      "Warte 8 Sekunden...\n",
      "4 von 1193 Durchgängen ausgeführt. 40 Artikel geladen.\n",
      "Warte 10 Sekunden...\n",
      "5 von 1193 Durchgängen ausgeführt. 50 Artikel geladen.\n",
      "Warte 4 Sekunden...\n",
      "6 von 1193 Durchgängen ausgeführt. 60 Artikel geladen.\n",
      "Warte 6 Sekunden...\n",
      "7 von 1193 Durchgängen ausgeführt. 70 Artikel geladen.\n",
      "Warte 5 Sekunden...\n",
      "8 von 1193 Durchgängen ausgeführt. 80 Artikel geladen.\n",
      "Warte 5 Sekunden...\n",
      "9 von 1193 Durchgängen ausgeführt. 90 Artikel geladen.\n",
      "Warte 4 Sekunden...\n",
      "10 von 1193 Durchgängen ausgeführt. 100 Artikel geladen.\n",
      "Daten wurden erfolgreich zwischengespeichert.\n",
      "Warte 4 Sekunden...\n",
      "11 von 1193 Durchgängen ausgeführt. 110 Artikel geladen.\n",
      "Warte 3 Sekunden...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     wait_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarte \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sekunden...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFehler bei der Anfrage:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Funktion zum Speichern der Daten\n",
    "def save_data(all_data, filename='lists/CNBC_articles.json'):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_data, file, indent=4)\n",
    "    print(\"Daten wurden erfolgreich zwischengespeichert.\")\n",
    "\n",
    "# URL und Basis-Parameter für die GET-Anfrage\n",
    "url = \"https://api.queryly.com/cnbc/json.aspx\"\n",
    "queryly_key = \"31a35d40a9a64ab3\"\n",
    "batch_size = 10  # Stellen Sie sicher, dass dies der Batchsize Ihrer Anfrage entspricht\n",
    "params = {\n",
    "    \"queryly_key\": queryly_key,\n",
    "    \"query\": \"boeing\",\n",
    "    \"batchsize\": batch_size,\n",
    "    \"callback\": \"\",\n",
    "    \"showfaceted\": \"false\",\n",
    "    \"timezoneoffset\": \"-60\",\n",
    "    \"facetedfields\": \"formats\",\n",
    "    \"facetedkey\": \"formats|\",\n",
    "    \"facetedvalue\": \"!Press Release|\",\n",
    "    \"sort\": \"date\",\n",
    "    \"additionalindexes\": \"4cd6f71fbf22424d,937d600b0d0d4e23,3bfbe40caee7443e,626fdfcd96444f28\"\n",
    "}\n",
    "\n",
    "# Initialanfrage, um die Gesamtzahl der Ergebnisse zu ermitteln\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    total_results = data['metadata']['totalresults']\n",
    "    total_requests_needed = math.ceil(total_results / batch_size)\n",
    "\n",
    "    all_data = []  # Sammeln aller Daten hier\n",
    "\n",
    "    for endindex in range(0, total_requests_needed * batch_size, batch_size):\n",
    "        # Parameter aktualisieren für jede Anfrage\n",
    "        params['endindex'] = endindex\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_data.extend(data['results'])\n",
    "            print(f\"{int(endindex/batch_size)+1} von {total_requests_needed} Durchgängen ausgeführt. {len(all_data)} Artikel geladen.\")\n",
    "            \n",
    "            # Speichern der Daten nach jeweils 100 geladenen Ergebnissen\n",
    "            if len(all_data) % 100 == 0 or (int(endindex/batch_size)+1) == total_requests_needed:\n",
    "                save_data(all_data)\n",
    "\n",
    "            # Warten für eine zufällige Zeit zwischen 2 und 10 Sekunden\n",
    "            wait_time = random.randint(2, 10)\n",
    "            print(f\"Warte {wait_time} Sekunden...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(\"Fehler bei der Anfrage:\", response.status_code)\n",
    "            break\n",
    "\n",
    "    # Speichern der gesammelten Daten in einer Datei\n",
    "    with open('lists/CNBC_articles.json', 'w') as file:\n",
    "        json.dump(all_data, file, indent=4)\n",
    "\n",
    "    print(\"Gesamte Daten wurden erfolgreich heruntergeladen und gespeichert.\")\n",
    "else:\n",
    "    print(\"Initialanfrage fehlgeschlagen:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8b3fa-06af-4901-a361-6259a1aae118",
   "metadata": {},
   "source": [
    "### Laden der einzelnen Artikel\n",
    "\n",
    "Da das vorherige Skript eine JSON mit allen Links zu den verfügbaren Artikeln erzeugt, gilt es nun die dort enthaltenen Artikel herunterzuladen.\n",
    "\n",
    "Die bereits erzeugte JSON enthält für jeden Artikel folgenden Eintrag, mit passendem Link zur Html Datei.\n",
    "\n",
    "```cn:liveURL\"https://www.cnbc.com/2024/03/22/faa-to-step-up-scrutiny-of-united-airlines-after-recent-incidents.html\"```\n",
    "\n",
    "\n",
    "In der HTML der Artikel ist eine JSON eingebettet, die Text des Artikels enthält. \n",
    "\n",
    "Die JSON ist dabei in der Variable ``` <script charSet=\"UTF-8\"> window.__s_data = {``` enthalten.\n",
    "    \n",
    "\n",
    "![alt text](screenshots/2.png \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badb908-bc67-4429-8f3c-1a7007ae8bca",
   "metadata": {},
   "source": [
    "Mittels BeautifulSoup und einer RegEx suche nach der genannten ```window.__s_data``` Variablen, wird die enthaltene JSNON extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d9de2e0-314c-4959-82b1-2362b4ed538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die JSON-Daten wurden erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "# URL des Artikels\n",
    "url = \"https://www.cnbc.com/2024/03/22/airbus-says-its-not-happy-about-issues-at-rival-boeing.html\"\n",
    "\n",
    "# Anfrage an die URL senden und Antwort erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfen, ob die Anfrage erfolgreich war\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text  # HTML-Inhalt der Antwort\n",
    "\n",
    "    # Erstellen einer BeautifulSoup-Instanz und Parsen des HTML-Inhalts\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Suche nach dem <script> Tag, der die gewünschten JSON-Daten enthält\n",
    "    script_tag = soup.find('script', string=re.compile('window\\.__s_data'))\n",
    "\n",
    "    if script_tag:\n",
    "        # Extrahieren des JavaScript-Codes als Text\n",
    "        script_content = script_tag.string\n",
    "\n",
    "        # Suchen des JSON-Teils im Script-Content mit einem regulären Ausdruck\n",
    "        json_data_match = re.search(r'window\\.__s_data\\s*=\\s*(\\{.*?\\});', script_content, re.DOTALL)\n",
    "\n",
    "        if json_data_match:\n",
    "            json_data_str = json_data_match.group(1)  # JSON als String\n",
    "\n",
    "            # Umwandlung des String in ein Python-Dictionary\n",
    "            json_data = json.loads(json_data_str)\n",
    "\n",
    "            # Speichern der JSON-Daten in einer Datei\n",
    "            with open('lists/extracted_json_data.json', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(\"Die JSON-Daten wurden erfolgreich gespeichert.\")\n",
    "        else:\n",
    "            print(\"JSON-Daten konnten im Script-Tag nicht gefunden werden.\")\n",
    "    else:\n",
    "        print(\"Entsprechender <script> Tag nicht gefunden.\")\n",
    "else:\n",
    "    print(f\"Fehler beim Abrufen der Seite: Statuscode {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedba298-67cb-4df7-86ed-13cb6485a6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
